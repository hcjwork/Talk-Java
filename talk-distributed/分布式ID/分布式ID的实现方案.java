1、UUID
UUID，通用唯一识别码，是基于随机数生成的全局唯一标识，占128位即16字节。
随机数是使用的16进制数，包含0-9、a-f、A-F三种字符。
UUID串通常表示为32个16进制数，分为5组，每组字符串间用“-”分隔。
例如：123e4567-e89b-12d3-a456-426614174000
第一组能表示16^8个，第二三四组都能表示16^4个，第五组能表示16^12个，
整个UUID串能表示16^32种不同的字符串，在理论上几乎不可能出现重复的。
但由于采用的是随机数，在极端的并行环境下，也是有可能发生碰撞的，但概率也极低。

java.util包中有提供UUID工具类，需要UUID串可通过UUID工具类获取，例如：
String uid = UUID.randomUUID().toString();

如果使用UUID作为分布式ID，几乎不用考虑重复风险，实现和使用起来比较简单，因为是业内的通用标识。
但由使用的是16进制的随机数生成，没有任何业务含义，也没有任何顺序性，索引效率低，
而且存储空间也大，每一个UUID串都占16字节，所占存储空间是一个长整型（占8字节）两倍。

所以如果是对顺序性要求不高的，也基本不会用来建立索引的，就可以用UUID，比如用于日志跟踪的TraceId。

2、数据库自增
利用数据库的主键自增特性生成唯一标识。
例如MySQL通过auto_increment指定主键自增，也可以在创建表时或创建表后指定自增的起始数和自增步长。

数据库自增的ID，简单可靠且严格递增，索引效率高。
但有单点故障风险，扩展性也差。

如果使用数据库主键自增来生成分布式ID，当使用集群模式，就算不同节点设置不同的起始数，
但还是很可能出现后生成的ID序号是小于现生成的ID的，这在业务逻辑顺序上其实不合预期，
而且超过2个节点，就没办法保证生成的ID不重复了，
除非一开始就给各个节点设置的起始数间隔足够大，比如一个从1开始，一个从1000万开始。
所以主要有三个方面的问题：顺序性问题、扩展性问题、间隔浪费问题。

所以数据库自增生成的ID不太适用于分布式系统，除非就用一个库来专门生成分布式ID，但可用性就大打折扣了。

3、号段模式
号段模式虽然是批量获取ID号段，降低了数据库的压力，
但与数据库自增方案有着同样的问题，比如单库影响可用性，集群模式下号段重复或浪费等。

号段模式时从数据库一次性批量获取ID段（如每次取1000个），存在内存中，逐个分配完后再次获取。
但因为使用的是数据库，数据库的瓶颈问题依然存在。
为了解决单点故障问题要考虑使用集群模式，
而集群模式如果采用的是主从架构，一旦出现机器宕机，因为同步的延迟性就可能造成号段冲突或浪费。
如果集群采用的是去中心化的，多节点独立缓存号段，节点崩溃时未使用的号段会永久浪费，
另外动态扩容时需要人工干预号段范围，会增加号段冲突和浪费。
当号段用尽时，申请新号段会进入阻塞等待，如果数据库响应慢，业务延迟会比较明显。
可以在号段将要用尽时预加载下一个号段，避免分配阻塞，如剩余20%时触发加载。

所以基于数据库的号段模式一般只用于低并发的单库场景中。

4、Redis原子自增
Redis有提供一种incr相关的原子递增指令，能产生全局递增的唯一性标识。
其中incr命令默认每次递增步长为1，也可以通过incrby命令指定递增步长。两种指令都是从0开始计算的。
例如：
incr distributed_id 
incrby distributed_id 5

Redis原子自增指令所产生的ID也有严格的顺序性，索引的效率高，生成速率也可观。
但需要引入Redis，如果是单机Redis通用也要面对单点故障问题，此时也需要使用集群来实现。

Redis集群支持通过一致性哈希算法将数据分片到不同节点（16384个槽，2^14），
相同键可以分配到同一节点，但如果是不同键可能分配到不同节点，
所以如果要想要保持全局递增，就需要只使用一个键，那么其实跟单机Redis没什么区别了。
可以考虑分片键+时间前缀，每天都更新分布式ID生成键，这样能尽量覆盖多的节点，避免某个节点压力过大。
还有种方案是Lua脚本+复合键，使用hash tag确保相关键在同一个节点。
但分片键+时间前缀更为简单性能也更高，还能尽可能覆盖多个节点，是推荐方案。
例如：
# 每日生成新key，结合年月日和分片ID
incr id:20230801:shard1

实际工程中如果真要使用Redis生成分布式ID，通常使用Redisson等成熟客户端封装，它们有提供更高层次的分布式原子计数器实现。
但在严格需要全局单调递增的场景，Redis集群也不是最佳选择，维护Redis集群需要较高的成本。

5、Snowflake算法
雪花算法是生成一个长整数作为分布式ID，
该长整数占8字节即64位，首位通常作为保留位，第2位到第42位这41位是记录时间戳，第43位到第52位这10位是用作记录机器码，
第53到64位这12位是用作序列号。
当然这些只是一般性的分配做法，保留位自然也是可以应用起来的。
| | ... | .... |  | ... |
1 2     42     52 53   64            

10位记录机器码可以表示2^10即1024个不同机器，如果只有几十个机器，选6位也就是2^6即64个已经绰绰有余了，剩下的5位可以都用作序列号。

41位用作时间戳标识，可以标识大约49年，不过系统时间戳是基于1971年1月1日格林尼治时间计算的，
实际工程可以将这个基础时间设置成指定时间，比如通过2025年7月1日转化出来的时间戳，
这样就可以标识2025年7月1日到2074年7月1日这49年，这完全足够了，一般的系统能够支撑个30年都已经是非常了不起了。
41位用作时间戳这是固定的，时间戳就是这么长的位数，也没办法调整了。

除去41位时间戳和6位的机器码，剩下还有17位可以用作序列号，即2^17为131072个序列号，这表示1毫秒能支撑131072个不同序列号，
因为时间戳是毫秒单位，换算到秒单位，1秒能支持131072000个即1亿多个不同的并发序列号生成，现在或者哪怕30年后都难以1亿的并发量，
先不说有没有这么多用户，单是服务就支撑不到这种程度。

如果机器数量较多，比如上万，就可以考虑从序列号部分匀出几位来增加机器码的类型数，比如一共用14位即能标识16384个不同的机器。
剩下11位作为序列号，能在1毫秒内标识2048个不同序列号，换算到秒单位1秒能标识2048000即200万的不同序列号，也完全够用了。

所以雪花算法目前是分布式ID实现方案中应用最广泛的，本身实现也不难，所占空间也不大只有64字节，且是一个长整数方便索引排序检索，
能支持的并发也高。

雪花算法也是有问题，因为涉及到时间戳，如果系统出现时间回拨问题，即系统被时钟同步器重置或人工重置，时间可能回到之前，
但之前时间的时间戳如果已经被用了，再根据相同时间戳生成的ID就有可能出现重复。
要解决时间回拨问题，如果是短时间回拨可以等待时钟追上或，如果是长时间回拨可以切换到WorkerID备用池。
像等待时间追上的方案可以参考美团Leaf的"等待策略"，备用WorkerID池切换可以参考百度的UidGenerator。

另外机器ID的分配，可以通过ZooKeeper/DB/配置文件管理，以避免冲突。

6、Leaf算法（号段模式+Snowflake算法）
支持号段和雪花ID双模式，号段优化为异步更新，采用等待策略解决短时间回拨。

7、UidGenerator算法
百度的UidGenerator算法也是Snowflake变种，通过备用WorkerID池切换解决长时间回拨问题。
结合了Cached模式，性能很好。

8、TinyID
基于号段模式，采用多DB轮询，通过多数据源分摊压力。