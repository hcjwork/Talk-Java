I/O多路复用是一种基于事件驱动的NIO实现。
其核心思路是在Socket连接建立时，为每个Socket连接生成唯一的文件描述符（FD），并将FD注册给内核，
在I/O操作发起后，监听FD的目标事件，当I/O状态变更时触发对应的回调处理。

I/O多路复用主要有select、poll、epoll三种类型。
select是最早的多路复用模型，时间复杂度是O(n)，需要遍历所有注册的Channel，性能较差，但适用于跨平台兼容场景（兼容旧版本平台）；
poll是select的改进版，但依然需要遍历所有FD，时间复杂度也是O(n)，解决了select的FD数量限制，poll没有上限；
epoll是Linux独有的高性能多路复用模型，时间复杂度是O(1)，基于事件驱动，仅返回就绪的FD，仅限Linux有，Windows使用select或IOCP。

select多路复用模型，对于FD的数量有限制，比如在Linux中select模型可监测的FD最大数量是1024。
如果是在Linux平台，Java NIO会自动选择性能最佳的epoll模型。

I/O多路复用，I/O Multiplexing，通过单线程监听多个文件描述符（File Descriptor，FD），当任意FD就绪（可读/可写）时
由内核通知监听者处理。
FD是操作系统（即内核）对I/O资源（如Socket、文件、管道等）的唯一标识，相当于一个操作句柄。
Socket是网络连接的一种I/O资源，创建后会被分配一个FD，通过FD可以操作Socket，比如：
read(fd, buf, size) → 从 Socket 收数据；
write(fd, buf, size) → 向 Socket 发数据；
epoll_ctl(epoll_fd, fd) → 监听 Socket 事件。

单线程监听所在服务的端口的相关Socket连接，比如当前服务的运行端口是8080，监听当前机器上所有8080端口的连接。
内核根据TCP五元组信息（源IP、源端口、目标IP、目标端口、协议）区分不同的连接，确保数据包路由到正确的进程。
应用程序仅处理指向当前进程的Socket连接，内核会自动过滤无关连接（如其他进程的8080端口连接）。
线程开启指定端口的连接监听后进入阻塞等待状态，内核维护连接列表，只要目标端口的连接中有出现FD状态变更的，就会唤醒阻塞线程，
线程遍历FD状态变更的连接，然后进行连接的不同状态做相应的处理。

一开始的I/O多路复用中，就是采用的select模型，在select模型中，内核只知道多少个FD的状态发送了变更，但不知道具体是哪些，
因此只要有个FD的状态有变更了就会唤醒监听者线程，因为内核也不知道具体是哪些FD的状态变更了，所以监听线程需要遍历所有
当前进程的指定端口的连接，分不同的FD状态做不同的处理，当然不同的FD状态要按逻辑顺序进行判断处理，比如acceptable状态
要先于readable状态进行判断处理。
在这种select模型里，FD有数量限制，由内核的FD_SETSIZE决定，通常为1024，因为其底层使用固定大小的位图（fd_set）
来存储监听的FD，超出位图范围的FD无法被处理。
这个fd_set中，每个bit表示一个FD，默认大小为1024位。
而select模型因为要遍历所有注册的socket连接，时间复杂度为O(n)，即便能支持更多的FD，性能也会随着FD数量线性下降。

后来的内核使用动态数组替代了select的固定位图（fd_set），通过链表式结构支持任意数量的FD监听，FD数量仅受系统内存限制，
所以理论上是无限的。这种I/O多路复用模型即为poll模型，解决了FD数量有限制的问题，但依然需要遍历所有指定端口的socket连接，
时间复杂度还是O(n)。所以也不适合高并发场景。

到了现代，高并发需求愈发迫切，为了提高I/O多路复用的效率，linux系统改善了内核对于FD的维护，不仅知道有多少个FD发生了变更，
还能知道具体是哪些FD有变更，基于内核这个能力，linux研发了epoll模型，在epoll模型中，内核只返回已经就绪的FD，
这样监听线程就不用遍历所有的Socket连接，只需要遍历已就绪的，在10000个并发连接某一个时刻准备就绪的可能就10个，因此
epoll的时间复杂度近乎O(1)，在高并发场景下的性能远高于select模型和poll模型。
当然epoll模型是linux系统提供的实现，在其他操作系统如macOS中提供的是kqueue，在Windows中则用异步的IOCP模型来提高性能。
kqueue也是基于事件通知，类似epoll，同样有极高的并发性能。

