现代分布式系统经常谈论“高并发、高性能、高可用”这三大关键特性，如果说如何实现这三大特性，通常会讲硬件资源、线程模型、线程池、 
锁、异步编程、缓存、中间件、分库分表、多实例部署、主从复制、集群化等技术点，但这些都是从具体的或者应用层面来探讨，
而对于这三高特性本身却很少关注，导致很多时候开发者的视角停留在了技术应用或所谓的技术方案等宏观层面。一谈到三高特性，每个开发者
好像都像套了一样模板的流水线工作者，但其实是否真的需要这些公式化的方案呢？是否能脱离这些模板而真正适配具体项目或规划的个性化需求呢？
或许除了宏观层面，还应该从微观层面，回到三高特性的本身来探索才行。

高，就是强、大、多，但满足高的同时也必然会牺牲其他方面，选型其实就是实际的具体需求做均衡抉择。
并发是什么？性能是什么？可用又是什么？理论层面的理解要深入，那具体工程中又是如何体现的呢？有哪些直观的或可视化的数据印证？
我们在实际工程所持续的生命周期里，有时会觉得高并发、高性能、高可用离得较远，有时又觉得息息相关或无处不在，但一时间却说不清
道不明。其实所有的这些技术名词都是为需求催生出来的，没有具体且必要的需求场景就没有对应技术被创造，
比如坐宇宙飞船围绕太阳表层飞行三圈，这种需求至少在目前是非必要的，那么就不会有人去实现这种技术。

并发、性能、可用有哪些直观的数据指标？
QPS、TPS、RT、延迟、吞吐量、并发线程数、3P/5P、CPU占用率、IO占用率、磁盘占用率等。

1、并发
并发是指系统在单位时间内处理任务的能力。不一定是物理并行，也可能是CPU时间片的超高速轮转。
并发的高低从另一个角度来看，就是单位时间内系统能处理完毕的任务有多少，这个单位时间可以是毫秒或秒，但对于程序开发者而言，
秒是更符合人类日常理解的时间单位，所以很多指标都以秒为基本时间单位来探讨。而对于计算机硬件资源，尤其是内存和CPU，
基本时间单位是微秒或纳秒或甚至更精细的时间。

以秒为单位时间，1秒内系统能完成的任务数，其实就是通俗意义上的并发数。
并发本身只是一种逻辑或物理行为，指的是多个线程在同一时刻同时执行或是在极短时间内交替执行，并不能直观地作为数据指标。
真正可以作为数据指标的是并发数或并发量，即单位时间内（通常以秒来探讨）系统能完成的任务的数量。

所谓的能力其实可以从两个方向来考量，一是完成一个任务所需要的时间，二是在单位时间内所能完成的任务的进度或数量。

每秒所能完成的查询请求的数量，称为QPS（Query Per Second）；
每秒所能完成的事务的数量，称为TPS（Transaction Per Second），一个事务可能包含多个查询。
虽然QPS的含义中是指的查询，但实际上平常程序开发者所关注的QPS指的是每秒从用户侧的客户端所发起的业务请求数，即覆盖了客户端、
服务层、数据层的请求响应完整流程的每秒内的请求数。

QPS如果从交互层面区分可以分为：服务层的QPS、数据库层的QPS。
服务层的QPS就是通常意义上的QPS，像压力测试、性能测试中所关注的就是服务层的包含请求响应完整流程的QPS；
数据库层的QPS则是数据库系统在每秒内所能完成的SQL查询数，至于在针对数据库系统的测试中才会加强关注。

TPS跟事务相关，那自然是数据库层的事情，服务层是不涉及到事务这种概念的。
一个事务是可以包含多个操作的，比如可能包含查询、修改、删除、新增等其中的多种操作，所以TPS其实也只是评估出数据库系统的
大致事务处理能力，毕竟每个事务可能包含DDL或DML操作也可能不包含。而且一个大事务的耗时可能会高于多个小事务的总共耗时。

其实从并发的含义来看就知道高并发不一定就代表多线程。现代CPU的处理速度非常快，为了更充分地利用CPU处理能力，
操作系统利用定时调度将CPU的处理从逻辑上划分成了无数个固定长度的时间片，CPU的时间片不断轮转，操作系统根据调度算法赋予
线程以CPU时间片进而执行应用程序。CPU时间片通常是固定的，要执行的程序任务耗时越短，单位时间内能完成的任务数就越多，即
程序的并发量就越高。现在CPU通常有多个核心，这多个核心可以在同一时刻同时各自独立执行程序任务，因此CPU的核数越多，程序
理论上能支持的并发量就越高。假设有一个16核CPU，程序任务的平均耗时为10ms，每核CPU每秒（1000ms）能处理的程序任务数为100，
那么16核同时工作每秒能处理的程序任务数就能达到1600，程序所支持的并发用户数最低为16（即这一核都是处理的同一个用户的连续100次请求），
程序所能支持的并发用户数最高位1600（即每一个程序任务都是来自不同的用户请求）。

程序中同一时刻同时运行的线程数与CPU的核数有关，假设一个CPU只有8核，那么程序中同时运行的线程数最高为8，超过8就必然有线程
在这一时刻时处于空闲状态。但如果是单位时间内也就是每秒内，程序中运行或说活跃的线程数不仅与CPU核数有关，还与CPU时间片的轮转有关。
但程序的最大线程数理论上与CPU时间片无关，而是与程序的运行内存有关，比如Java程序线程所占空间被称为线程栈，
默认每个线程分配一个1MB大小的线程栈存储运行时的各种数据，而Java线程与操作系统线程又是一对一的关系，
每创建一个Java线程在内核中就会对应创建一个内核线程，在Linux内核中一个内核线程默认分配一个8MB大小的线程栈。
所以程序每秒内所能支持的活跃线程数，也就是并发线程数，受限于程序的运行内存，还受限于内核线程，包括CPU时间轮转时线程上下文
的切换开销。
假设程序的运行内存最大为4GB，程序的最大线程数理论上可达到4096，但实际可能连1000都达不到，因为程序的其他方面也需要运行内存。
另外程序可支持的线程数高并不代表程序的并发量高，因为线程数量大时线程上下文切换比如阻塞、唤醒、程序计数器切换等操作所需的时间
就更长，那么单位时间内所能完成的任务的数量自然就会更低，程序的并发量就越小，所以实际工程里程序的线程数的控制，要结合
CPU的核数、线程上下文开销，还需要考虑任务的计算操作与I/O操作的比例，尤其是线程池中线程数的设置，任务中计算操作多是应尽量
与CPU核数靠近，因为计算操作执行起来快，任务中I/O操作多时则一般设置为CPU核数的两倍，因为磁盘或网络交互操作耗时长，此时
CPU时间片可以让出给其他线程。

还有一个数据指标跟并发有关的就是吞吐量，吞吐量是指系统在单位时间内所能处理的数据量，其实跟并发量是正向关的，单位时间内
能处理的数据量越大，分到到任务量上自然也就越多，即单位时间内能处理的平均任务量越多。

总结与并发相关的数据指标为：QPS、TPS、并发任务数、并发用户数、并发线程数、吞吐量。

如何提高并发？

2、性能
性能是指系统完成任务的效率。高性能通常体现为任务耗时短、资源消耗少。
单位时间内完成的任务量越多性能越高，单个任务所消耗的时间越少性能越高。

并发和性能其实是相类似的含义，两者是强烈的正相关关系。
性能越高就意味着单位时间内所能处理的任务数越多，即并发越高；
换言之，并发越高，就代表着系统处理任务的耗时越短，即性能越高。

性能关键数据指标：响应时间（RT，Response Time）、吞吐量（Throughput）、资源利用率。
响应时间：请求从发出到收到响应整个流程所占用的时间。也可以说是延迟时间。
吞吐量：系统每秒内处理的数据量。这里的数据可以指的数据本身，也可指的是任务数、请求数。
资源利用率：CPU、内存、磁盘、网络等资源的利用率。

通常来说延迟其实就等同于响应时间，客户端发出一个请求后，经过一段时间才会收到响应结果，这段时间就是响应时间或延迟时间。
对于客户端或程序开发者而言，这个延迟当然越低越好，当然前提是请求被成功处理并返回了预期内的响应结果。
吞吐量理论则是越高越好，越高代表系统能处理的量越大，不管这个量是数据量、请求量还是任务量。
假设系统的处理能力是恒定的，单位时间内能处理的请求量也会是恒定的，不是说接收到的请求量系统都能处理得过来，如果超过了系统
的处理能力，就会引起系统拒绝能力外的请求甚至出现崩溃，也就是说一个系统是有其请求承载能力上限的。

单论延迟和吞吐量这两个指标的对比，延迟低通常也意味着吞吐量高，但并非绝对。
但其实吞吐量要看是什么的吞吐量，一种是请求的吞吐量，一种是I/O的吞吐量。请求的吞吐量就是指的系统在每秒内所能处理的请求数，这跟服务层的QPS差不多。IO的吞吐量，包含网络IO和磁盘IO，
所谓的IO就是指的针对内存的输入输出，从内存外传输数据到内存中称为输入，从内存中传输数据到内存外称为输出，网络和磁盘都是
属于内存外。这样分请求吞吐量和I/O吞吐量好像也没有必要，因为请求必然涉及到将客户端发送的数据从网络从输入到内存里进行解析，
这就已经涉及到网络I/O了。

延迟是系统处理单个请求所需要的时间，吞吐量是系统在单位时间内所能处理的请求数。
延迟延迟，不就是时间差吗？谁的时间差，响应结果接收时间点与请求发出时间点的时间差；
吞吐吞吐，吞不就是输入，吐不就是输出吗？谁的输入和输出？当然是系统，但系统又不是容器，所以是系统的运行内存的输入和输出。
什么在输入输出？当然是数据，不管数据是以什么协议、什么格式传输。

但如果将单个请求从发出到收到响应是以客户端的角度来说的，请求从客户端发送再到客户端接收到响应，这部分时间包括了两段网络
传输时间，一段是请求从客户端到服务端的网络传输，一段是响应从服务端到客户端的网络传输，程序员通常所说的网络延迟就是指的
数据从一端传输到另一端的所需时间。严格上来讲，这两段网络延迟不应该算在服务端延迟里，但对于客户端来说，我请求发出去了
到我接受到该请求的响应，这中间的时间差都是服务端的延迟，有网络延迟那是你们服务端需要解决的事情，所以网络传输的带宽、
传输协议、数据格式等通常都是服务端进行约定，客户端作为服务消费者，服务端作为服务提供者，消费者要以提供者为基准，当然
也可以双方协调调整，只要约定统一即可。

如果只论系统的延迟，则只算系统接收请求到发出响应的这段时间。

请求数据从客户端传输过来时是一个整体，那么由整体到整体就没什么意义，但由整体到分散则可以缩短处理时间。比如一个请求划分
成三个子任务同时处理，再把结果汇总起来响应给客户端。这就是以多线程分摊来降低延迟的策略，但这个线程数量要结合CPU核数和
线程上下文切换开销来衡量。

1）如何降低系统延迟
采用高效的序列化方式和数据传输格式，缩短数据解析时间。

将任务分成多个子任务，采用多线程分摊执行。

优化代码以缩短代码执行时间，比如提高算法能力、减少I/O操作。

使用高效的I/O技术执行I/O操作以缩短I/O操作耗时，比如使用零拷贝技术省去数据用户态到内核态的拷贝。

使用缓存代替直接查询数据库，提高缓存命中率，能缩短请求处理时间。本地缓存和缓存数据库结合，缩短数据查询时间。

使用异步解耦将耗时工作交给子线程或消息队列处理，主线程快速返回响应。

使用批量操作代替循环单操作，缩短数据库交互时间。

为数据库表字段建立必要索引，缩短数据库查询时间。比如对经常查询的字段、关联的字段、排序的字段建立合理的索引。

对JVM进行调优，减少GC停顿时间和GC频次。比如调整堆的初始大小和最大大小、调整方法区的大小。

返回数据最小化原则，比如分页返回、只返回必要的字段。

2）如何提高系统吞吐量
一般情况下降低了延迟就能提高吞吐量，但在处理大数据量时，可以将数据收集到一定量时要进行下一步处理，牺牲少量的延迟可增加
大额吞吐量。
另外还可以通过多线程或多服务实例来提高吞吐量。

3）如何降低CPU使用率
关于CPU的使用率，最好的状态是CPU达到100%但不超过100%，这几乎是没法做到的，所以一般来说CPU使用率稳定在90%作用就差不多了。
如果CPU使用率突然飙高到100%，那就说明有的程序对CPU时间片的占用比例过多，如果是在Linux中可以通过`top`命令查看详情。

程序使用CPU比率上升明显，通常是因为程序中执行到了频繁使用CPU的代码，比如Thread.sleep操作、while或for无限循环、
深层次嵌套while或for循环、深层次递归调用等。如果是执行到I/O代码，则会是I/O使用率上升而不是CPU使用率上升。
除了上面的这些频繁的非I/O操作，计算机密集型的任务本身也会引起CPU使用率上升，比如数据加解密、序列化和反序列化。
而对于Java程序，JVM的频繁GC也会导致CPU使用率飙高。锁竞争也会导致CPU使用率飙高，无论是JVM锁还是分布式锁。

说白了，就是非I/O代码执行多了就会导致CPU使用率飙高。

如何降低CPU使用率？
减少无意义的代码，比如减少循环嵌套深度、减少方法嵌套深度；
使用更高效的算法来完成计算密集型任务，比如更高效的加解密算法、更高效的序列化与反序列化算法；
将耗时操作交给线程池或消息队列，通过异步化来降低CPU的持续占用时间；
降低锁竞争，尽量采用无锁编程来解决并发冲突；
优化JVM，选用更高效的垃圾回收器，降低GC频次；

4）如何降低I/O占用率
I/O包含网络I/O和磁盘I/O两部分。
网络I/O指的是网络和内存间的数据流转，磁盘I/O指的是磁盘和内存间的数据流转。

网络I/O性能与网络带宽、网络连接有关，减少连接数、增加带宽、降低带宽占用量即可以提高网络I/O性能。
比如使用I/O多路复用模型可减少连接数，减少传输数据量和压缩传输数据可降低带宽。

磁盘I/O性能与磁盘类型、文件系统、操作系统内核等有关。
比如使用SSD代替普通磁盘以提高随机读写性能，调整文件系统以提高文件读写性能，调优内核参数控制刷盘策略。

通过提高网络I/O和磁盘I/O的性能可以降低I/O占用率。代码中也应当尽量减少I/O操作的比例。
零拷贝技术也可以降低I/O占用率。

5）如何降低磁盘占用率
磁盘的占用率其实与文件的大小和数量有关，比如日志文件、话单文件、账单文件等，如果不及时清理则会越积越多。
业务性的数据文件可能保留一个月或数个月，但日志文件通常保留7天并保持滚动更新。
如果是重要的涉及审计或回溯的日志，通常记录在日志表中并定时备份存储。

在Linux中磁盘使用率可以通过`df -h`命令查看。
各种软件的安装比如缓存、数据库、消息队列等的安装也会占用磁盘空间，安装完成后可以将安装包挪到统一的目录存储，
如果磁盘占用率过高可酌情清除一部分安装包。

磁盘空间不够将会导致系统异常或崩溃，比如数据库表数据无法新增、导出文件无法生成、临时存储文件无法生成。
如果不是特殊要求，应当定期清理各类文件。在Linux中可以通过crontab定时清除无用文件，尤其是日志文件、临时文件。

磁盘占用率超过70%时需要警惕，应及时清理或扩展磁盘。

6）如何降低内存占用率
内存是程序得以正常运行的关键资源，没有足够内存程序会出现异常甚至崩溃。
对于Java程序，本地缓存可以降低系统延迟，但需注意要及时清理以释放部分内存。
对于常用的池比如线程池、连接池、对象池等在使用完毕后要酌情关闭，比如sftp连接释放、socket连接释放等。
避免内存泄漏，比如ThreadLocal保证线程并发安全的同时要注意在使用完毕后通过remove方法释放无用的ThreadLocalMap元素。
调整JVM的堆最大大小和方法区最大大小，避免内存占用率过高。
合理选择GC策略或GC器，以降低GC时的内存占用。

3、可用
可用是指系统持续提供服务的能力。可用性通常也与故障率关联讨论，故障率越低就代表可用性越高。
可用性的常见问题比如功能异常、单节点故障、服务链路雪崩，都是由于服务的各种故障引起。
这些故障可能是代码层面的，可能是网络层面的，也可能是服务节点层面的。

加强代码的逻辑准确性、健壮性、安全性是保障系统可用性的基本工作，此外多实例部署、冗余备份、熔断降级、异地多活也是有效手段。
多实例部署：服务在多个节点部署或在一个节点部署多个实例，在部分服务实例宕机时还有其他服务实例可以保障系统的可用性。
冗余备份：通过数据副本提供服务灾难备份，避免服务实例宕机后完全失去服务可用性，可以快速切换备份恢复完整可用性。
熔断降级：当请求量超过服务的负载上限时，为避免冲垮服务，采用服务降级策略，返回缓存数据或统一处理结果，以保障服务的基本可用。
    在长调用链路中，如果涉及到其他系统出现了异常或服务不可用，及时熔断调用链路以避免异常蔓延到自身系统。
异地多活：采用单元化架构在不同的地点部署多个服务实例。




